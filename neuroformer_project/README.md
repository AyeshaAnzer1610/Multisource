# NeuroFormer: Transformer-Based Multimodal Integration for Mental Health Diagnosis

[![Python 3.9](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.12](https://img.shields.io/badge/pytorch-1.12-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Official implementation of "Transformer-Based Multimodal Integration for Diagnostic Modelling of Mental Health" published in IEEE Access 2025.

## ðŸ“‹ Overview

NeuroFormer is a transformer-based multimodal deep learning framework that integrates three complementary, non-invasive data streams for automated mental health disorder detection:
- **EEG (Electroencephalography)**: 5-channel neural activity
- **Eye-tracking**: Oculomotor patterns and pupil dynamics
- **Behavioral Data**: Task performance metrics

### Key Features

- **80.3% Accuracy** on CMI dataset (85.1% with auxiliary losses)
- **79-83% Zero-shot Transfer** across 5 external datasets
- **Interpretable Attention** visualizations for clinical insights
- **Low-cost sensors** (<$2000 total equipment cost)
- **Robust Generalization** across diverse populations and conditions

## ðŸ—ï¸ Architecture

```
Input: EEG (5ch) + Eye-tracking (96 features) + Behavioral (64 features)
  â†“
Modality-Specific Encoders (4 layers, 8 heads each)
  â†“
Cross-Modal Fusion Transformer (6 layers, 8 heads)
  â†“
Classification Head (MLP with dropout)
  â†“
Output: Healthy Control vs. Mental Health Disorder
```

## ðŸ“¦ Installation

### Requirements
- Python 3.9+
- CUDA 11.3+ (for GPU acceleration)
- 16GB+ RAM
- NVIDIA GPU with 8GB+ VRAM (recommended)

### Setup

```bash
# Clone repository
git clone https://github.com/yourusername/neuroformer.git
cd neuroformer

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package in development mode
pip install -e .
```

## ðŸ“Š Dataset

### Child Mind Institute (CMI) Healthy Brain Network

The primary dataset used in this work is the [CMI Healthy Brain Network](http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/).

**Download Instructions:**
1. Visit the CMI data portal
2. Accept data use agreement
3. Download the following modalities:
   - EEG recordings (`.edf` format)
   - Eye-tracking data (`.asc` format)
   - Behavioral task data (`.csv` format)
   - Clinical assessments (K-SADS, CGI-S)

**Dataset Structure:**
```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ eeg/
â”‚   â”‚   â”œâ”€â”€ sub-001_task-gonogo_eeg.edf
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ eyetracking/
â”‚   â”‚   â”œâ”€â”€ sub-001_task-gonogo_eyetrack.asc
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ behavioral/
â”‚   â”‚   â”œâ”€â”€ sub-001_task-gonogo_beh.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ phenotypic/
â”‚       â””â”€â”€ participants.csv
â””â”€â”€ processed/
    â””â”€â”€ (generated by preprocessing scripts)
```

### External Validation Datasets

The following datasets were used for cross-dataset generalization:
- **MODMA**: Depression detection
- **ADHD-200**: ADHD classification
- **COBRE**: Schizophrenia detection
- **DEAP**: Emotion recognition (valence)
- **EmotionNet**: Emotion recognition (arousal)

## ðŸš€ Quick Start

### 1. Data Preprocessing

```bash
# Preprocess all modalities
python scripts/preprocess_data.py \
    --data_dir data/raw \
    --output_dir data/processed \
    --config configs/preprocessing_config.yaml
```

### 2. Training

```bash
# Train NeuroFormer (base model)
python scripts/train.py \
    --config configs/neuroformer_config.yaml \
    --data_dir data/processed \
    --output_dir experiments/neuroformer_base

# Train NeuroFormer++ (with auxiliary losses)
python scripts/train.py \
    --config configs/neuroformer++_config.yaml \
    --data_dir data/processed \
    --output_dir experiments/neuroformer_plus
```

### 3. Evaluation

```bash
# Evaluate on test set
python scripts/evaluate.py \
    --checkpoint experiments/neuroformer_base/best_model.pth \
    --data_dir data/processed \
    --output_dir results/

# Cross-dataset evaluation
python scripts/cross_dataset_eval.py \
    --checkpoint experiments/neuroformer_base/best_model.pth \
    --external_datasets data/external/ \
    --output_dir results/cross_dataset/
```

### 4. Visualization

```bash
# Generate attention visualizations
python scripts/visualize_attention.py \
    --checkpoint experiments/neuroformer_base/best_model.pth \
    --data_dir data/processed \
    --output_dir visualizations/

# Plot training curves
python scripts/plot_training.py \
    --log_dir experiments/neuroformer_base/logs \
    --output_dir figures/
```

## ðŸ“ˆ Reproducing Paper Results

### Main Results (Table 2)

```bash
# Run all baseline models
bash scripts/run_all_baselines.sh

# Run NeuroFormer variants
bash scripts/run_neuroformer_variants.sh

# Generate results table
python scripts/generate_results_table.py \
    --experiments_dir experiments/ \
    --output results/main_results.csv
```

### Ablation Studies (Table 3)

```bash
# Modality ablation
python scripts/ablation_modalities.py \
    --config configs/ablation_config.yaml \
    --output results/ablation_modalities.csv

# Architectural ablation
python scripts/ablation_architecture.py \
    --config configs/ablation_config.yaml \
    --output results/ablation_architecture.csv
```

### Cross-Dataset Generalization (Table 5)

```bash
# Zero-shot transfer to external datasets
python scripts/cross_dataset_eval.py \
    --checkpoint experiments/neuroformer_base/best_model.pth \
    --external_datasets data/external/ \
    --mode zero_shot \
    --output results/cross_dataset_zero_shot.csv

# Fine-tuning on external datasets
python scripts/cross_dataset_eval.py \
    --checkpoint experiments/neuroformer_base/best_model.pth \
    --external_datasets data/external/ \
    --mode finetune \
    --finetune_ratio 0.2 \
    --output results/cross_dataset_finetuned.csv
```

## ðŸ“ Repository Structure

```
neuroformer/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ neuroformer_config.yaml
â”‚   â”œâ”€â”€ neuroformer++_config.yaml
â”‚   â”œâ”€â”€ preprocessing_config.yaml
â”‚   â””â”€â”€ ablation_config.yaml
â”œâ”€â”€ data/                       # Data directory (not in repo)
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ external/
â”œâ”€â”€ neuroformer/               # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                # Model implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ neuroformer.py
â”‚   â”‚   â”œâ”€â”€ encoders.py
â”‚   â”‚   â”œâ”€â”€ fusion.py
â”‚   â”‚   â””â”€â”€ baselines.py
â”‚   â”œâ”€â”€ data/                  # Data processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”‚   â”œâ”€â”€ eeg_preprocessing.py
â”‚   â”‚   â”‚   â”œâ”€â”€ eyetracking_preprocessing.py
â”‚   â”‚   â”‚   â””â”€â”€ behavioral_preprocessing.py
â”‚   â”‚   â””â”€â”€ augmentation.py
â”‚   â”œâ”€â”€ training/              # Training utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ visualization.py
â”‚       â”œâ”€â”€ logging.py
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ scripts/                   # Executable scripts
â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ cross_dataset_eval.py
â”‚   â”œâ”€â”€ ablation_modalities.py
â”‚   â”œâ”€â”€ ablation_architecture.py
â”‚   â”œâ”€â”€ visualize_attention.py
â”‚   â””â”€â”€ run_all_baselines.sh
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing_demo.ipynb
â”‚   â”œâ”€â”€ 03_model_analysis.ipynb
â”‚   â””â”€â”€ 04_visualization.ipynb
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â””â”€â”€ test_dataset.py
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ setup.py                   # Package setup
â”œâ”€â”€ LICENSE                    # MIT License
â””â”€â”€ README.md                  # This file
```

## ðŸŽ¯ Performance Summary

| Model | Accuracy | Precision | Recall | F1-Score | AUROC |
|-------|----------|-----------|--------|----------|-------|
| **NeuroFormer** | 80.3Â±2.3% | 79.7Â±2.7% | 81.2Â±2.9% | 80.4Â±2.4% | 0.867Â±0.019 |
| **NeuroFormer++** | 85.1Â±1.9% | 84.4Â±2.3% | 86.1Â±2.5% | 85.2Â±2.0% | 0.912Â±0.015 |

### Cross-Dataset Generalization (Zero-shot)
| Dataset | Target | Accuracy | AUROC |
|---------|--------|----------|-------|
| MODMA | Depression | 81.1% | 0.874 |
| ADHD-200 | ADHD | 79.3% | 0.846 |
| COBRE | Schizophrenia | 83.3% | 0.891 |
| DEAP | Emotion | 79.7% | 0.853 |
| EmotionNet | Emotion | 81.3% | 0.867 |

## ðŸ”¬ Key Findings

1. **Eye-tracking provides primary discriminative power** (74.8% accuracy alone)
2. **Bimodal Eye+Behavior achieves 79.8-83.9%** (not significantly different from tri-modal)
3. **Sparse EEG (5-channel) contributes minimally** in this configuration
4. **Robust cross-dataset transfer** (79-83% across 5 external datasets)
5. **Interpretable attention patterns** align with neurocognitive processing stages

## ðŸ› ï¸ Advanced Usage

### Custom Configuration

```python
# configs/custom_config.yaml
model:
  name: "NeuroFormer"
  d_model: 256
  n_encoder_layers: 4
  n_fusion_layers: 6
  n_heads: 8
  dropout: 0.3
  
modalities:
  eeg:
    enabled: true
    n_channels: 5
    sampling_rate: 500
  eyetracking:
    enabled: true
    sampling_rate: 60
  behavioral:
    enabled: true

training:
  batch_size: 32
  learning_rate: 0.0001
  weight_decay: 0.00001
  epochs: 60
  early_stopping_patience: 10
```

### Python API

```python
from neuroformer.models import NeuroFormer
from neuroformer.data import MultimodalDataset
from neuroformer.training import Trainer

# Load data
dataset = MultimodalDataset(
    data_dir="data/processed",
    modalities=["eeg", "eyetracking", "behavioral"]
)

# Initialize model
model = NeuroFormer(
    d_model=256,
    n_encoder_layers=4,
    n_fusion_layers=6,
    n_heads=8,
    dropout=0.3
)

# Train
trainer = Trainer(
    model=model,
    train_dataset=dataset.train,
    val_dataset=dataset.val,
    config=config
)
trainer.train()

# Evaluate
results = trainer.evaluate(dataset.test)
print(f"Test Accuracy: {results['accuracy']:.3f}")
```

## ðŸ“Š Computational Requirements

| Component | Specification |
|-----------|--------------|
| Parameters | 15.7M (base), 18.9M (++) |
| Training Time | 2.2 hrs/fold (base), 2.7 hrs/fold (++) |
| Inference Time | 15.8 ms/sample (GPU), 180 ms (CPU) |
| Memory | 5.1 GB GPU, 8 GB RAM |

**Hardware Used:**
- GPU: NVIDIA RTX 3090 (24 GB VRAM)
- CPU: AMD Ryzen 9 5950X
- RAM: 64 GB

## ðŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@article{anzer2025neuroformer,
  title={Transformer-Based Multimodal Integration for Diagnostic Modelling of Mental Health},
  author={Anzer, Ayesha and Bais, Abdul},
  journal={IEEE Access},
  year={2025},
  volume={},
  pages={},
  doi={10.1109/ACCESS.2025.0429000}
}
```

## ðŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Code formatting
black neuroformer/
flake8 neuroformer/

# Type checking
mypy neuroformer/
```

## ðŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- Child Mind Institute for providing the Healthy Brain Network dataset
- The open-source community for excellent tools and libraries
- Reviewers and collaborators for valuable feedback

## ðŸ“§ Contact

- **Ayesha Anzer** - aag833@uregina.ca
- **Abdul Bais** - abdul.bais@uregina.ca

For questions, issues, or collaboration opportunities, please open an issue on GitHub or contact us directly.

## ðŸ”— Links

- [Paper](https://ieeexplore.ieee.org/document/xxxxx)
- [CMI Dataset](http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/)
- [Supplementary Materials](https://github.com/yourusername/neuroformer/tree/main/supplementary)

---

**Note:** This is research code. While we have made every effort to ensure correctness and reproducibility, clinical deployment requires additional validation, regulatory approval, and ethical considerations.
